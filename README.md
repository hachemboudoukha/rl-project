# ğŸ¤– Reinforcement Learning Project - Modular Framework

Ce projet est une bibliothÃ¨que complÃ¨te d'apprentissage par renforcement (RL) implÃ©mentÃ©e "from scratch". Il regroupe les principaux algorithmes classiques (Programmation Dynamique, Monte Carlo, Temporal Difference, Planning) et une variÃ©tÃ© d'environnements de test, incluant des environnements "secrets" pour l'Ã©valuation de la robustesse des agents.

---

## ğŸ¯ Objectifs du Projet

- **ModularitÃ©** : SÃ©paration stricte entre les algorithmes, les environnements et les politiques.
- **ExtensibilitÃ©** : FacilitÃ© d'ajout de nouveaux agents ou mondes grÃ¢ce Ã  des interfaces de base (`BaseAgent`, `BaseEnvironment`).
- **ExpÃ©rimentation** : Outils pour comparer les performances, Ã©tudier la convergence et l'impact des hyperparamÃ¨tres ($\alpha, \gamma, \epsilon$).
- **Visualisation** : Rendu console et graphique des performances des agents.

---

## ğŸ“‚ Architecture du Projet

```text
rl-project/
â”œâ”€â”€ rl/
â”‚   â”œâ”€â”€ algorithms/          # ImplÃ©mentations des algorithmes RL
â”‚   â”‚   â”œâ”€â”€ dynamic_programming/  # Policy/Value Iteration
â”‚   â”‚   â”œâ”€â”€ monte_carlo/         # ES, On-policy, Off-policy
â”‚   â”‚   â”œâ”€â”€ temporal_difference/ # SARSA, Q-Learning, Expected SARSA
â”‚   â”‚   â””â”€â”€ planning/            # Dyna-Q, Dyna-Q+
â”‚   â”œâ”€â”€ environments/        # Mondes RL (Standard & Secrets)
â”‚   â”œâ”€â”€ policies/            # StratÃ©gies d'action (Greedy, Epsilon-Greedy)
â”‚   â”œâ”€â”€ utils/               # Metrics, Logger, Visualisation, SÃ©rialisation
â”‚   â””â”€â”€ experiments/         # Scripts pour lancer des tests massifs
â”œâ”€â”€ demo/                    # Scripts interactifs et visualisation
â”œâ”€â”€ saved_models/            # Sauvegarde des politiques entraÃ®nÃ©es (.pkl)
â”œâ”€â”€ reports/                 # Graphiques et rÃ©sultats d'expÃ©riences
â”œâ”€â”€ main.py                  # Point d'entrÃ©e principal
â””â”€â”€ requirements.txt         # DÃ©pendances Python
```

---

## ğŸ§  Algorithmes ImplÃ©mentÃ©s

### ğŸ”¹ Programmation Dynamique
*UtilisÃ©s quand le modÃ¨le (P, R) est connu.*
- **Policy Iteration** : Ã‰valuation et amÃ©lioration itÃ©rative de la politique.
- **Value Iteration** : Convergence directe vers la fonction de valeur optimale.

### ğŸ”¹ MÃ©thodes Monte Carlo
*Apprentissage par Ã©pisodes complets.*
- **Monte Carlo ES** (Exploring Starts) : Garantie d'exploration de tous les Ã©tats.
- **On-policy First-Visit MC** : Apprentissage direct sur la politique cible.
- **Off-policy MC** : Utilisation de l'importance sampling pour apprendre une politique cible via une politique de comportement.

### ğŸ”¹ Temporal Difference (TD) Learning
*Apprentissage en ligne (step-by-step).*
- **SARSA** : On-policy, plus sÃ»r pendant l'apprentissage.
- **Q-Learning** : Off-policy, converge vers la politique optimale.
- **Expected SARSA** : Utilise l'espÃ©rance mathÃ©matique pour rÃ©duire la variance.

### ğŸ”¹ Planning
- **Dyna-Q** : Combine apprentissage rÃ©el et simulations internes (modÃ¨le).
- **Dyna-Q+** : IntÃ¨gre un bonus de curiositÃ© pour dÃ©couvrir de nouvelles opportunitÃ©s.

---

## ğŸŒ Environnements

| Environnement | Description | IntÃ©rÃªt |
| :--- | :--- | :--- |
| **Line World** | Monde 1D simple | Validation des bases. |
| **Grid World** | Monde 2D | Comparaison classique des perfs. |
| **Two-round RPS** | Chifoumi en 2 tours | DÃ©pendance temporelle & stratÃ©gie adverse. |
| **Monty Hall (L1)** | ProblÃ¨me des 3 portes | Apprentissage de stratÃ©gies contre-intuitives. |
| **Monty Hall (L2)** | 5 portes, multi-Ã©tapes | Test de scalabilitÃ©. |
| **Secret Envs** | Envs (0-3) inconnus | Test de robustesse et gÃ©nÃ©ralisation. |

---

## ğŸš€ Installation & Utilisation

### Installation
```bash
pip install -r requirements.txt
```

### ğŸ® Mode Manuel (Jouer soi-mÃªme)
Testez les rÃ¨gles d'un environnement :
```bash
python3 demo/play_manual.py
```

### ğŸ—ï¸ EntraÃ®nement & ExpÃ©rimentation
Lancez un entraÃ®nement par dÃ©faut via `main.py` :
```bash
python3 main.py
```

### ğŸ•µï¸ Tester les Environnements Secrets
Utilisez le script dÃ©diÃ© pour tester les environnements fournis par l'enseignant :
```bash
python3 demo/test_secret_envs.py [0-1-2-3]
```

### ğŸ“º Rejouer une Politique SauvegardÃ©e
```bash
python3 demo/replay_policy.py gridworld saved_models/q_values/qlearning_gridworld.pkl
```

---

## ğŸ“Š Visualisation
Le framework inclut des outils pour gÃ©nÃ©rer :
- Les **Courbes d'apprentissage** (Reward cumulÃ© par Ã©pisode).
- Les **Heatmaps de valeur** (V-tables pour les Grids).
- Les **Logs dÃ©taillÃ©s** dans le dossier `logs/`.

---

## ğŸ› ï¸ Comment ajouter un composant ?

### Nouvel Environnement
HÃ©ritez de `BaseEnvironment` dans `rl/environments/base_env.py` et implÃ©mentez `reset()`, `step()`, `get_actions()`, `get_states()` et `render()`.

### Nouvel Algorithme
HÃ©ritez de `BaseAgent` dans `rl/algorithms/base_agent.py` et implÃ©mentez `train()` et `act()`.

---

> **Note** : Ce projet a Ã©tÃ© conÃ§u pour Ãªtre le plus lisible possible afin de faciliter la rÃ©daction du rapport technique final.
